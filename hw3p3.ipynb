{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Problem 3 Classification Comparison with Synthetic Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Write a jupyter notebook. \n",
    "* It is essential that you include markdown cells explaining what you are doing at each step.\n",
    "\n",
    "* You are going to compare the classification algorithms you have learned:\n",
    "\n",
    "1.  Naive Bayes Classification\n",
    "2.  Logistic Regression\n",
    "3.  Quadradic Discriminant Analysis\n",
    "4.  SVM using radial basis functions (RBF)\n",
    "5.  Decision Tree\n",
    "6.  KNN with K = 1\n",
    "\n",
    "So make sure you run each of these algorithms on the data sets you will create to test.\n",
    "\n",
    "* The goal is to create 4 synthetic data examples with 2D input data, 2 classes, to use to compare the algorithms. \n",
    "* You should use a combination of the following 3 synthetic functions to make your examples:\n",
    "-   [sklearn.datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs)\n",
    "-   [sklearn.datasets.make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles)\n",
    "-   [sklearn.datasets.make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons)\n",
    "\n",
    "* You may use a combination of the above in the same example. For example, one class could come from a moon and the other class from a circle in the same example. You could also make one class is a combination of a moon bending oneway and a moon bending the other if that worked better for your example.\n",
    "* Your examples should be such that:\n",
    "\n",
    "1.  One of the four examples will be two well-separated blob classes. This example show that all the classifiers work pretty well (near-perfect or perfect classification)\n",
    "2.  In the two of the data examples, the linear classifier (logistic regression) should do more poorly (both in training and test evaluation) than the others\n",
    "3.  In the second example the quadratic should do very well (both in training and testing) but the linear classifier should do poorly. The SVM, Decision Tree, and KNN should also do better than the Logistic Regressioon\n",
    "4.  In the third example you should be able to construct an example where the decision tree and KNN both overfit so the training error is small but the testing error is large, and linear regression does better\n",
    "5.  The fourth example should be such that SVM and KNN do better than the others (in testing error)\n",
    "\n",
    "In your evaluations, you should always split the data set into a training and test set using [sklearn.model_selection.train_test_splitÂ ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) and also create a report using [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n",
    "\n",
    "There should be both an evaluation for training which means fit on training test on training (evaluation of fit) and testing which means fit on training but test on the testing data with the training fit!\n",
    "Never, ever, fit on training data!!!\n",
    "\n",
    "Since you are working with 2D data you should also create a set of visualized comparisons like those here:\n",
    "\n",
    "<div\n",
    "data-ephox-embed-iri=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\"\n",
    "style=\"max-width: 650px; border: 1px solid #aaaaaa; box-shadow: rgba(0, 0, 0, 0.14) 0px 2px 2px 0px, rgba(0, 0, 0, 0.2) 0px 3px 1px -2px, rgba(0, 0, 0, 0.12) 0px 1px 5px 0px; padding: 10px; overflow: hidden; margin-bottom: 1em;\">\n",
    "<p><a\n",
    "href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\"\n",
    "style=\"text-decoration: none; color: inherit;\"><img\n",
    "src=\"https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\"\n",
    "style=\"max-width: 180px; max-height: 180px; margin-left: 2em; float: right;\" /></a>\n",
    "<a\n",
    "href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\"\n",
    "style=\"text-decoration: none; color: inherit;\"><span\n",
    "style=\"font-size: 1.2em; display: block;\">Classifier comparison</span>\n",
    "<span style=\"margin-top: 0.5em; display: block;\">A comparison of a\n",
    "several classifiers in scikit-learn on synthetic datasets. The point of\n",
    "this example is to illustrate the nature of decision boundaries of\n",
    "different classifiers. This should be ta...</span> <span\n",
    "style=\"color: #999999; display: block; margin-top: 0.5em;\">scikit-learn</span></a></p>\n",
    "</div>\n",
    "\n",
    "You can use these visualizations and comparisons as inspiration but you should develop your own. Don\\'t forget to see your random numbers! For some more idea on how to generate the synthetic data please read these examples too:\n",
    "\n",
    "<div\n",
    "data-ephox-embed-iri=\"https://scikit-learn.org/stable/modules/clustering.html#clustering\"\n",
    "style=\"max-width: 650px; border: 1px solid #aaaaaa; box-shadow: rgba(0, 0, 0, 0.14) 0px 2px 2px 0px, rgba(0, 0, 0, 0.2) 0px 3px 1px -2px, rgba(0, 0, 0, 0.12) 0px 1px 5px 0px; padding: 10px; overflow: hidden; margin-bottom: 1em;\">\n",
    "<p><a\n",
    "href=\"https://scikit-learn.org/stable/modules/clustering.html#clustering\"\n",
    "style=\"text-decoration: none; color: inherit;\"><img\n",
    "src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\"\n",
    "style=\"max-width: 180px; max-height: 180px; margin-left: 2em; float: right;\" /></a>\n",
    "<a\n",
    "href=\"https://scikit-learn.org/stable/modules/clustering.html#clustering\"\n",
    "style=\"text-decoration: none; color: inherit;\"><span\n",
    "style=\"font-size: 1.2em; display: block;\">2.3. Clustering</span> <span\n",
    "style=\"margin-top: 0.5em; display: block;\">Clustering of unlabeled data\n",
    "can be performed with the module sklearn.cluster. Each clustering\n",
    "algorithm comes in two variants: a class, that implements the fit method\n",
    "to learn the clusters on trai...</span> <span\n",
    "style=\"color: #999999; display: block; margin-top: 0.5em;\">scikit-learn</span></a></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs,make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building synthetic data from case 1\n",
    "x_case1,y_case1=make_blobs(n_samples=300,centers=2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blob function is used to generate seperated blob classes. It generates isotropic gaussian blobs for clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building synthetic data from case 2\n",
    "x_case2, y_case2=make_circles(n_samples=300, noise=0.2, factor=0.4, random_state=42)\n",
    "x_moon, y_moon=make_moons(n_samples=100, noise=0.2, random_state=42)\n",
    "x_case2=np.vstack([x_case2,x_moon])\n",
    "y_case2=np.hstack([y_case2, y_moon])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case, we are trying to create complex data by combining both the circles and moon data.We generate the circle data first and then generate the moons data and then concatenate both along their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building synthetic data from case 3\n",
    "x_case3, y_case3=make_moons(n_samples=300, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show quadratic seperation, we the make_moon function to generate the moon data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building synthetic data from case 4\n",
    "x_case4, y_case4=make_circles(n_samples=300, noise=0.2, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we create complex data where svm and knn are expected to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting all the synthetic data in a list\n",
    "data_list=[(x_case1,y_case1), (x_case2, y_case2), (x_case3, y_case3), (x_case4, y_case4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to define classifiers\n",
    "classifiers={\"Naive Bayes\": GaussianNB(), \"logistic regression\": LogisticRegression(), \"decision tree\": DecisionTreeClassifier(), \"knn with k=1\":KNeighborsClassifier(n_neighbors=1), \"quadratic discriminant analysis\": QuadraticDiscriminantAnalysis(), \"svm\":SVC(kernel=\"rbf\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are creating the instances of the classifiers we want to use and put putting them all in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
